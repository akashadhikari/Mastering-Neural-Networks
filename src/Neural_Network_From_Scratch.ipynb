{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lets Make A Neural Network From Scratch!<center>\n",
    "<div style=\"text-align: right\"> - by <b><a href=\"https://github.com/akashadhikari/\">Akash</a></b> (Not the one in the picture below, obviously! :p )</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/feynman.png\" alt=\"Drawing\" style=\"width: 1000px; height: 400px\"/>\n",
    "<b>Richard Feynman</b><br>Picture credit: [Commonlounge](https://www.commonlounge.com/discussion/cd6812822aef4dd7a718b420c4fa4036/history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: No Machine Learning Framework or Libraries were 'harmed' while making this tutorial.\n",
    "And before you ask, Yes, Frameworks make things a lot easier. But trust me, if you really want to GET IT from the core, you MUST be able to grasp the theoretical base of Neural Networks and implement a toy Neural Net of your own. Yes, without ANY libraries. That is how you will learn. The Hard Freaking Way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem:\n",
    "We begin with a very simple problem with a very limited data.<br>\n",
    "Below, we have a set of random data points plotted in a 2 dimensional graph with axis X(B) and axis Y(A).\n",
    "Some data points are coloured/labeled as red and some are labeled blue.\n",
    "(Seeing the graph, you brain has already recognized the pattern and classified these data points in a certain way. You have to admit that our brain is very good at pattern recognition.)<br>\n",
    "Our job is to create a neural network, feed this data to the network and train it so that it can correctly classify the new (test) data points with reasonable accuracy. Thus, we are making a neural network classifier to simply understand what's going on inside each layer and not just see the network as a \"black box\".\n",
    "For testing, a random data point will be given (say, 2, -3). Our neural network classifier will identify the label of the given data point (either Red or Blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification](images/data_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of our network\n",
    "\n",
    "### Data format\n",
    "Our data points can simply be represented in a 2 dimensional graph with coordinates (X1 and X2) and label Y. In the table below, the labels of Y are either 0 and 1. 0 indicates blue (mostly in left part of the graph) and 1 indicated red colour of the coordinate (mostly situated at the right part).\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "|--- |:--:|--:|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 0 |\n",
    "| 1  | 0  | 1 |\n",
    "| -3 | 2  | 0 |\n",
    "| ...|... |...|\n",
    "\n",
    "### Constructing our neural network with this data\n",
    "Constructing the network is easy. We have a mandatory <b>Input</b> and the <b>Output</b> layer.\n",
    "There should be 2 input neurons to accept the input data as the data points are defined by X1 and X2 Similarly, we have to define 1 output neuron as the classification is binary.\n",
    "We will use 1 hiddne layer with 4 neurons. You can play with the model later by increasing the hidden layer neurons or even the layer itself.\n",
    "![Architecture](images/architecture.png)\n",
    "\n",
    "### Weights / Synapses\n",
    "First set of connection: Weight matrix has 2 X 4 dimensions (Input neurons X Hidden layer neurons).<br>\n",
    "Second set of connection: Weight matrix has 4 X 1 dimensions (Hidden layer neurons X Output layer neurons).\n",
    "\n",
    "### Activation function and sigmoid\n",
    "An activation function maps the input values in between 0 to 1 or -1 to 1, etc. (depending upon the function used). Here, we are using a sigmoid function that looks like an 'S' curve stretching from - infinity to + infinity and the values ranging from 0 to 1.\n",
    "\n",
    "s(x) = 1/(1+e^(-x))\n",
    "![Sigmoid](images/sigmoid.gif)\n",
    "\n",
    "### What does that mean?\n",
    "It means, if we pass any number to the above \"nonlin\" function (let's say 5 and -5 as in the above figure), the output should come accordingly.\n",
    "```\n",
    "nonlin(5) # Check the figure, the value (red curve) is closer to 1 and so is our output.\n",
    "0.9933071490757153\n",
    "\n",
    "nonlin(5) # Check the figure, the value (red curve) is closer to 0 and so is our output.\n",
    "0.9933071490757153\n",
    "```\n",
    "### The SUMMARIZED process.\n",
    "- We feed the input training set to the neural network.\n",
    "- We initialize a random weights and perform **forward propagation**.\n",
    "- With that process, the network gives some output. We compare this output with the actualy output label (Y).\n",
    "- The difference we get is the error we obtain in that particular iteration.\n",
    "- We **backpropagate** to reduce the error by changing the weights by small margin.\n",
    "- We continue the iteration unless the error is really low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just give me the damn code.\n",
    "### (It won't be a bad idea to blindly (with your eyes wide open) copy-paste this code snippet and try tweaking things on your own. If not, skip this code and stick with me for the rest of the part).\n",
    "**PS: Don't forget to ``` pip3 install numpy``` if you haven't already.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, number_of_inputs_per_neuron, number_of_neurons):\n",
    "        self.synaptic_weights = 2 * np.random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def sigmoid_activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def sigmoid_activation_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # The neural network forward_propagates.\n",
    "    def forward_propagate(self, inputs):\n",
    "        output_from_layer1 = self.sigmoid_activation(np.dot(inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.sigmoid_activation(np.dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network\n",
    "            output_from_layer_1, output_from_layer_2 = self.forward_propagate(training_set_inputs)\n",
    "\n",
    "            # Calculate the error for layer 2 (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            layer2_error = training_set_outputs - output_from_layer_2\n",
    "            layer2_delta = layer2_error * self.sigmoid_activation_derivative(output_from_layer_2)\n",
    "\n",
    "            # Calculate the error for layer 1 (By looking at the weights in layer 1,\n",
    "            # we can determine by how much layer 1 contributed to the error in layer 2).\n",
    "            layer1_error = np.dot(layer2_delta, self.layer2.synaptic_weights.T)\n",
    "            layer1_delta = layer1_error * self.sigmoid_activation_derivative(output_from_layer_1)\n",
    "\n",
    "            # Calculate how much to adjust the weights by\n",
    "            layer1_adjustment = np.dot(training_set_inputs.T, layer1_delta)\n",
    "            layer2_adjustment = np.dot(output_from_layer_1.T, layer2_delta)\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.layer1.synaptic_weights += layer1_adjustment\n",
    "            self.layer2.synaptic_weights += layer2_adjustment\n",
    "            if iteration % 10000 == 0:\n",
    "                print(\"Training:\" , int(iteration/1000), \"%\")\n",
    "            if iteration == 99999:\n",
    "                print(\"Training complete - 100%!\")\n",
    "\n",
    "    # The neural network prints its weights\n",
    "    def print_weights(self):\n",
    "        print (\"    Layer 1 (4 neurons, each with 2 inputs): \")\n",
    "        print (self.layer1.synaptic_weights)\n",
    "        print (\"    Layer 2 (1 neuron, with 4 inputs):\")\n",
    "        print (self.layer2.synaptic_weights)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Seed the random number generator\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Create layer 1 (4 neurons, each with 2 inputs)\n",
    "    layer1 = Layer(2, 4)\n",
    "\n",
    "    # Create layer 2 (a single neuron with 4 inputs)\n",
    "    layer2 = Layer(4, 1)\n",
    "\n",
    "    # Combine the layers to create a neural network\n",
    "    neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "    print (\"Stage 1) Random starting synaptic weights: \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    input_array = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [-1, 0],\n",
    "    [-2, -2],\n",
    "    [-2, -1],\n",
    "    [-1, -1],\n",
    "    [1, -1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [2, 0],\n",
    "    [3, 0],\n",
    "    [0, -3],\n",
    "    [0, -2],\n",
    "    [2, -2]])\n",
    "    output_array = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]).T\n",
    "\n",
    "    # The training set. We have 7 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = input_array\n",
    "\n",
    "    training_set_outputs = output_array\n",
    "    # Train the neural network using the training set.\n",
    "    # Do it 60,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 100000)\n",
    "\n",
    "    print (\"Stage 2) New synaptic weights after training: \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    # Test the neural network with a new situation.\n",
    "    new_test = np.array([2, -3])\n",
    "    hidden_state, output = neural_network.forward_propagate(new_test)\n",
    "    print (\"Stage 3) Considering a new situation\", new_test)\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be patient and bear with me! That might look complicated but it's not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE WE GO!\n",
    "\n",
    "We start off by creating a simple work-flow. That is, we will make a skeleton of the program without even declaring workable class and methods.\n",
    "\n",
    "To begin with, lets seed random numbers (which is not mandatory but it will make things easier for later usage as this function will seed the random values and will create consistency). Read more [here](https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do/21494630#21494630).\n",
    "### Seed it!\n",
    "\n",
    "```\n",
    "random.seed(1)\n",
    "```\n",
    "### We will now create Neural Network layers (In this case, layer1 (hidden layer) and layer2 (output layer)).\n",
    "```\n",
    "# Create layer 1 (4 neurons, each with 2 inputs)\n",
    "\n",
    "layer1 = Layer(2, 4)\n",
    "\n",
    "# Create layer 2 (a single neuron with 4 inputs)\n",
    "\n",
    "layer2 = Layer(4, 1)\n",
    "```\n",
    "#### Don't worry about calling the ``` Layer() ``` class. We haven't even built one. We will define it shortly.\n",
    "### Now, take the two layers and pass it to the non-existent ```NeuralNetwork()``` class and finally print the initial weights using ```print_weights()``` for analysis.\n",
    "\n",
    "```\n",
    "# Combine the layers to create a neural network\n",
    "\n",
    "neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "print (\"Stage 1) Random starting synaptic weights: \")\n",
    "\n",
    "neural_network.print_weights()\n",
    "```\n",
    "### Now, take the input data and train it using our non-existant ```train()``` function.\n",
    "```\n",
    "input_array = np.array([[0, 0],[0, 1],[0, 2],[-1, 0],[-2, -2],[-2, -1],[-1, -1],\n",
    "                         [1, -1],[1, 0],[1, 1],[2, 0],[3, 0],[0, -3],[0, -2],[2, -2]])\n",
    "output_array = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]).T\n",
    "\n",
    "training_set_inputs = input_array\n",
    "\n",
    "training_set_outputs = output_array\n",
    "\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 100000)\n",
    "```\n",
    "#### We are iterating over 100000 times so that the weights gets gradually updated to find the most appropriate final weight.\n",
    "### Print the new synaptic weights after training. Notice, the change in weights after these many iterations.\n",
    "```\n",
    "print (\"Stage 2) New synaptic weights after training: \")\n",
    "\n",
    "neural_network.print_weights()\n",
    "```\n",
    "### Test the network with new test situation and Forward Propagate with newly updated weights.\n",
    "```\n",
    "# Test the neural network with a new situation.\n",
    "\n",
    "new_test = np.array([2, -3])\n",
    "\n",
    "hidden_state, output = neural_network.forward_propagate(new_test)\n",
    "\n",
    "print (\"Stage 3) Considering a new situation\", new_test)\n",
    "\n",
    "print (output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far, our Neural Network looks somethig like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "layer1 = Layer(2, 4) # Layer() class is not defined yet.\n",
    "\n",
    "layer2 = Layer(4, 1)\n",
    "\n",
    "neural_network = NeuralNetwork(layer1, layer2) # NeuralNetwork() class is not defined yet.\n",
    "\n",
    "print (\"Stage 1) Random starting synaptic weights: \")\n",
    "neural_network.print_weights() # print_weights() method is not defined yet.\n",
    "\n",
    "input_array = np.array([[0, 0],[0, 1],[0, 2],[-1, 0],[-2, -2],[-2, -1],[-1, -1],\n",
    "                     [1, -1],[1, 0],[1, 1],[2, 0],[3, 0],[0, -3],[0, -2],[2, -2]])\n",
    "output_array = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]).T\n",
    "\n",
    "training_set_inputs = input_array\n",
    "\n",
    "training_set_outputs = output_array\n",
    "\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 100000) # train() method is not defined yet.\n",
    "\n",
    "print (\"Stage 2) New synaptic weights after training: \")\n",
    "neural_network.print_weights()\n",
    "\n",
    "new_test = np.array([2, -3])\n",
    "hidden_state, output = neural_network.forward_propagate(new_test) # forward_propagate() method isn't defined yet.\n",
    "print (\"Stage 3) Considering a new situation\", new_test)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to define all the Classes and methods\n",
    "#### ```class Layer()```\n",
    "We first define a ``` class Layer() ``` which has an init method.\n",
    "```\n",
    "class Layer():\n",
    "    def __init__(self, number_of_inputs_per_neuron, number_of_neurons):\n",
    "        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "```\n",
    "Here, we are simply constructing the weight matrices that accepts a set of randomly initialzed numbers with the dimensions (as talked earlier) **previous layer neurons X current layer neurons**. This is equavalent to saying  ``` number_of_inputs_per_neuron ``` and ``` number_of_neurons ```. Multiplication by 2 and subtraction by 1 is to maintain the synaptic weights witin a certain range of 'normalized' mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``` class NeuralNetwork() ```\n",
    "We define a ``` class NeuralNetwork() ``` which has **six** methods.\n",
    "\n",
    "```\n",
    "def __init__(self, layer1, layer2):\n",
    "    self.layer1 = layer1\n",
    "    self.layer2 = layer2\n",
    "```\n",
    "We initialize the NeuralNetwork() class by creating the layer variables.\n",
    "\n",
    "```\n",
    "\n",
    "def sigmoid_activation(self, x):\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "def sigmoid_activation_derivative(self, x):\n",
    "    return x * (1 - x)\n",
    "```\n",
    "While you must be familiar with the ``` sigmoid_activation ``` method, the derivative of the sigmoid (``` sigmoid_activation_derivative ```) function gives the slope of any point in the sigmoid curve.\n",
    "In other words, this is the gradient of the Sigmoid curve and it indicates how confident we are about the existing weight.\n",
    "\n",
    "```\n",
    "def forward_propagate(self, inputs):\n",
    "    output_from_layer1 = self.sigmoid_activation(dot(inputs, self.layer1.synaptic_weights))\n",
    "    output_from_layer2 = self.sigmoid_activation(dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "    return output_from_layer1, output_from_layer2\n",
    "```\n",
    "Forward propagation is the process of \"moving forward\" the network. In other words, we mutiply the weights and neuron layers and finally apply the sigmoid activation to regulate the obtained result. We do this till we obtain the final output.<br>\n",
    "\n",
    "And, finally, we define the ``` train ``` method to declare forward propagation, compare output results and apply backpropagation to update weights so that the network performs better in the next iterations.<br>\n",
    "Below, we have:<br>\n",
    "``` layer2_error ``` - which is just the difference of resulting output from our actual output.<br>\n",
    "``` layer2_delta ``` - which is the product of error and gradient of the output. This helps in gradual weight change.<br>\n",
    "And same with layer1.<br>\n",
    "Ultimately, the weight adjustement can be done by:\n",
    "```\n",
    "# Calculate how much to adjust the weights by\n",
    "layer1_adjustment = training_set_inputs.T.np.dot(layer1_delta)\n",
    "layer2_adjustment = output_from_layer_1.T.np.dot(layer2_delta)\n",
    "\n",
    "# Adjust the weights.\n",
    "self.layer1.synaptic_weights += layer1_adjustment\n",
    "self.layer2.synaptic_weights += layer2_adjustment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are you now? Not Atlantis! :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now put things together and see the neural network operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1) Random starting synaptic weights: \n",
      "    Layer 1 (4 neurons, each with 2 inputs): \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[-0.20646505]\n",
      " [ 0.07763347]\n",
      " [-0.16161097]\n",
      " [ 0.370439  ]]\n",
      "Training: 0 %\n",
      "Training: 10 %\n",
      "Training: 20 %\n",
      "Training: 30 %\n",
      "Training: 40 %\n",
      "Training: 50 %\n",
      "Training: 60 %\n",
      "Training: 70 %\n",
      "Training: 80 %\n",
      "Training: 90 %\n",
      "Training complete - 100%!\n",
      "Stage 2) New synaptic weights after training: \n",
      "    Layer 1 (4 neurons, each with 2 inputs): \n",
      "[[-3.97185157  4.66297249 -5.25188655  1.39753303]\n",
      " [ 1.35299999 -1.57439556  1.76665557 -0.71676559]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[ -7.20493687]\n",
      " [  6.57701463]\n",
      " [-11.30181144]\n",
      " [  0.89289527]]\n",
      "Stage 3) Considering a new situation [ 2 -3]\n",
      "[0.99942671]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, number_of_inputs_per_neuron, number_of_neurons):\n",
    "        self.synaptic_weights = 2 * np.random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def sigmoid_activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def sigmoid_activation_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # The neural network forward_propagates.\n",
    "    def forward_propagate(self, inputs):\n",
    "        output_from_layer1 = self.sigmoid_activation(np.dot(inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.sigmoid_activation(np.dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network\n",
    "            output_from_layer_1, output_from_layer_2 = self.forward_propagate(training_set_inputs)\n",
    "\n",
    "            # Calculate the error for layer 2 (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            layer2_error = training_set_outputs - output_from_layer_2\n",
    "            layer2_delta = layer2_error * self.sigmoid_activation_derivative(output_from_layer_2)\n",
    "\n",
    "            # Calculate the error for layer 1 (By looking at the weights in layer 1,\n",
    "            # we can determine by how much layer 1 contributed to the error in layer 2).\n",
    "            layer1_error = np.dot(layer2_delta, self.layer2.synaptic_weights.T)\n",
    "            layer1_delta = layer1_error * self.sigmoid_activation_derivative(output_from_layer_1)\n",
    "\n",
    "            # Calculate how much to adjust the weights by\n",
    "            layer1_adjustment = np.dot(training_set_inputs.T, layer1_delta)\n",
    "            layer2_adjustment = np.dot(output_from_layer_1.T, layer2_delta)\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.layer1.synaptic_weights += layer1_adjustment\n",
    "            self.layer2.synaptic_weights += layer2_adjustment\n",
    "            if iteration % 10000 == 0:\n",
    "                print(\"Training:\" , int(iteration/1000), \"%\")\n",
    "            if iteration == 99999:\n",
    "                print(\"Training complete - 100%!\")\n",
    "\n",
    "    # The neural network prints its weights\n",
    "    def print_weights(self):\n",
    "        print (\"    Layer 1 (4 neurons, each with 2 inputs): \")\n",
    "        print (self.layer1.synaptic_weights)\n",
    "        print (\"    Layer 2 (1 neuron, with 4 inputs):\")\n",
    "        print (self.layer2.synaptic_weights)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Seed the random number generator\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Create layer 1 (4 neurons, each with 2 inputs)\n",
    "    layer1 = Layer(2, 4)\n",
    "\n",
    "    # Create layer 2 (a single neuron with 4 inputs)\n",
    "    layer2 = Layer(4, 1)\n",
    "\n",
    "    # Combine the layers to create a neural network\n",
    "    neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "    print (\"Stage 1) Random starting synaptic weights: \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    input_array = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [-1, 0],\n",
    "    [-2, -2],\n",
    "    [-2, -1],\n",
    "    [-1, -1],\n",
    "    [1, -1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [2, 0],\n",
    "    [3, 0],\n",
    "    [0, -3],\n",
    "    [0, -2],\n",
    "    [2, -2]])\n",
    "    output_array = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]).T\n",
    "\n",
    "    # The training set. We have 7 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = input_array\n",
    "\n",
    "    training_set_outputs = output_array\n",
    "    # Train the neural network using the training set.\n",
    "    # Do it 60,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 100000)\n",
    "\n",
    "    print (\"Stage 2) New synaptic weights after training: \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    # Test the neural network with a new situation.\n",
    "    new_test = np.array([2, -3])\n",
    "    hidden_state, output = neural_network.forward_propagate(new_test)\n",
    "    print (\"Stage 3) Considering a new situation\", new_test)\n",
    "    print (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOM!\n",
    "The new situation (2,-3) is predicted as 0.999 which is about 1.<br>\n",
    "This mean, the point (2,-3) on the graph should be labeled red (Y label 1 is for red as mentioned above), which is pretty much what we have expected.<br>\n",
    "Still unconvinced? Try with different points and analyze what works and what doesn't. The more you explore, the more you will be able to understand the inner mechanisms of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We have just built our very own tiny neural network ALL from scratch.<br>\n",
    "It might not be the best network out there but it WORKS! And hopefuly now, you have come to know all the nuts and bolts of a simple multi-layered neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps?\n",
    "It's completely up to you. How far do you want to go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets get in touch!\n",
    "The name is **Akash Adhikari**.<br> \n",
    "**Address**- 221B Baker Street<br>\n",
    "No, wait!<br>\n",
    "**Address**: To and forth Kathmandu, Biratnagar or just about everywhere.<br>\n",
    "Contact me:<br>\n",
    "    **Facebook**: fb.com/akashbrt<br>\n",
    "    **Github**  : github.com/akashadhikari<br>\n",
    "    **website** : akashadhikari.com.np<br>\n",
    "    **email**   : akashsky1313@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
